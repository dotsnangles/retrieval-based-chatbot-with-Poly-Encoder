{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "poly encoder training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNo/eNwqIx3RLDv52+ncouy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dotsnangles/Retrieval-Based-Chatbot/blob/main/poly_encoder_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAcEd4LC1XkQ",
        "outputId": "b0014a0a-f604-4141-a520-65ffc6d7c894"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.4 MB 7.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 101 kB 11.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 54.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 45.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5-LfZdk1Iep"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertPreTrainedModel, BertModel\n",
        "\n",
        "\n",
        "class PolyEncoder(BertPreTrainedModel):\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super().__init__(config, *inputs, **kwargs)\n",
        "        self.bert = kwargs['bert']\n",
        "        self.poly_m = kwargs['poly_m']\n",
        "        self.poly_code_embeddings = nn.Embedding(self.poly_m, config.hidden_size)\n",
        "        # https://github.com/facebookresearch/ParlAI/blob/master/parlai/agents/transformer/polyencoder.py#L355\n",
        "        torch.nn.init.normal_(self.poly_code_embeddings.weight, config.hidden_size ** -0.5)\n",
        "\n",
        "    def dot_attention(self, q, k, v):\n",
        "        # q: [bs, poly_m, dim] or [bs, res_cnt, dim]\n",
        "        # k=v: [bs, length, dim] or [bs, poly_m, dim]\n",
        "        attn_weights = torch.matmul(q, k.transpose(2, 1)) # [bs, poly_m, length]\n",
        "        attn_weights = F.softmax(attn_weights, -1)\n",
        "        output = torch.matmul(attn_weights, v) # [bs, poly_m, dim]\n",
        "        return output\n",
        "\n",
        "    def forward(self, context_input_ids, context_input_masks,\n",
        "                            responses_input_ids, responses_input_masks, labels=None):\n",
        "        # during training, only select the first response\n",
        "        # we are using other instances in a batch as negative examples\n",
        "        if labels is not None:\n",
        "            responses_input_ids = responses_input_ids[:, 0, :].unsqueeze(1)\n",
        "            responses_input_masks = responses_input_masks[:, 0, :].unsqueeze(1)\n",
        "        batch_size, res_cnt, seq_length = responses_input_ids.shape # res_cnt is 1 during training\n",
        "\n",
        "        # context encoder\n",
        "        ctx_out = self.bert(context_input_ids, context_input_masks)[0]  # [bs, length, dim]\n",
        "        poly_code_ids = torch.arange(self.poly_m, dtype=torch.long).to(context_input_ids.device)\n",
        "        poly_code_ids = poly_code_ids.unsqueeze(0).expand(batch_size, self.poly_m)\n",
        "        poly_codes = self.poly_code_embeddings(poly_code_ids) # [bs, poly_m, dim]\n",
        "        embs = self.dot_attention(poly_codes, ctx_out, ctx_out) # [bs, poly_m, dim]\n",
        "\n",
        "        # response encoder\n",
        "        responses_input_ids = responses_input_ids.view(-1, seq_length)\n",
        "        responses_input_masks = responses_input_masks.view(-1, seq_length)\n",
        "        cand_emb = self.bert(responses_input_ids, responses_input_masks)[0][:,0,:] # [bs, dim]\n",
        "        cand_emb = cand_emb.view(batch_size, res_cnt, -1) # [bs, res_cnt, dim]\n",
        "\n",
        "        # merge\n",
        "        if labels is not None:\n",
        "            # we are recycling responses for faster training\n",
        "            # we repeat responses for batch_size times to simulate test phase\n",
        "            # so that every context is paired with batch_size responses\n",
        "            cand_emb = cand_emb.permute(1, 0, 2) # [1, bs, dim]\n",
        "            cand_emb = cand_emb.expand(batch_size, batch_size, cand_emb.shape[2]) # [bs, bs, dim]\n",
        "            ctx_emb = self.dot_attention(cand_emb, embs, embs).squeeze() # [bs, bs, dim]\n",
        "            dot_product = (ctx_emb*cand_emb).sum(-1) # [bs, bs]\n",
        "            mask = torch.eye(batch_size).to(context_input_ids.device) # [bs, bs]\n",
        "            loss = F.log_softmax(dot_product, dim=-1) * mask\n",
        "            loss = (-loss.sum(dim=1)).mean()\n",
        "            return loss\n",
        "        else:\n",
        "            ctx_emb = self.dot_attention(cand_emb, embs, embs) # [bs, res_cnt, dim]\n",
        "            dot_product = (ctx_emb*cand_emb).sum(-1)\n",
        "            return dot_product"
      ]
    }
  ]
}