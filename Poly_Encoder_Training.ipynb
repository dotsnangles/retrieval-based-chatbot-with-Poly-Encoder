{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Poly Encoder Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN1/znMaq1I3jgi+NIveaJD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dotsnangles/Retrieval-Based-Chatbot/blob/main/Poly_Encoder_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnEJHV_6YR4k",
        "outputId": "0dcc82af-9c41-4747-ab7a-9e04ceedd27d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jun 25 02:29:42 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets folium==0.2.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-ZyBZPWfX5m",
        "outputId": "9110b175-7ba9-4ab4-a7c3-fe0581ca25e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.3.2)\n",
            "Collecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.5.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79808 sha256=b24eda7d80b19f77a3123279320057e7446ed7a840631a9d17986e538a4d5a3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "Successfully installed folium-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4lrYnocYNH4",
        "outputId": "0df6ecd5-ede5-43fc-bff7-a0a12aff6704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Poly-Encoder'...\n",
            "remote: Enumerating objects: 74, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 74 (delta 6), reused 11 (delta 4), pack-reused 57\u001b[K\n",
            "Unpacking objects: 100% (74/74), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dotsnangles/Poly-Encoder.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2020_02_20/all_bert_models.zip\n",
        "!unzip /content/all_bert_models.zip -d /content/all_bert_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCM3W9NdYfyd",
        "outputId": "1fe994b6-f067-4b26-f748-618e92ab9e68"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-25 02:31:23--  https://storage.googleapis.com/bert_models/2020_02_20/all_bert_models.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.197.128, 74.125.135.128, 74.125.142.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.197.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2941379946 (2.7G) [application/zip]\n",
            "Saving to: ‘all_bert_models.zip’\n",
            "\n",
            "all_bert_models.zip 100%[===================>]   2.74G   135MB/s    in 21s     \n",
            "\n",
            "2022-06-25 02:31:45 (134 MB/s) - ‘all_bert_models.zip’ saved [2941379946/2941379946]\n",
            "\n",
            "Archive:  /content/all_bert_models.zip\n",
            " extracting: /content/all_bert_models/uncased_L-10_H-128_A-2.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-10_H-256_A-4.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-10_H-512_A-8.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-10_H-768_A-12.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-12_H-128_A-2.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-12_H-256_A-4.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-12_H-512_A-8.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-12_H-768_A-12.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-2_H-128_A-2.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-2_H-256_A-4.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-2_H-512_A-8.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-2_H-768_A-12.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-4_H-128_A-2.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-4_H-256_A-4.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-4_H-512_A-8.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-4_H-768_A-12.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-6_H-128_A-2.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-6_H-256_A-4.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-6_H-512_A-8.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-6_H-768_A-12.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-8_H-128_A-2.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-8_H-256_A-4.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-8_H-512_A-8.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-8_H-768_A-12.zip  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/all_bert_models/uncased_L-4_H-512_A-8.zip -d /content/Poly-Encoder/bert_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIYCevHedHxf",
        "outputId": "8cf57d12-4382-425f-c551-9e67752d1d35"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/all_bert_models/uncased_L-4_H-512_A-8.zip\n",
            "  inflating: /content/Poly-Encoder/bert_model/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: /content/Poly-Encoder/bert_model/bert_config.json  \n",
            "  inflating: /content/Poly-Encoder/bert_model/vocab.txt  \n",
            "  inflating: /content/Poly-Encoder/bert_model/bert_model.ckpt.index  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --folder 1uC1pSCrh9xlieF60z78QuCg7OxvU9kUs\n",
        "!mv /content/dstc7/*.json /content/Poly-Encoder/dstc7\n",
        "!mv /content/dstc7/*.tsv /content/Poly-Encoder/dstc7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edrW8JmxZWrb",
        "outputId": "ab9ab6ee-4ac8-415e-890a-7f44f631128b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder list\n",
            "Processing file 1YXsj5U-P1nj3ID2ni62vvW4m_4AMYh_v ubuntu_dev_subtask_1.json\n",
            "Processing file 1I46tBMSYrDCFb0UoequBSl4Uqo0GOVms ubuntu_responses_subtask_1.tsv\n",
            "Processing file 1tHAe_WGFqQUHmdM28RttlVsYukL69qtg ubuntu_test_subtask_1.json\n",
            "Processing file 1s2Fz0wQD-YL0tR14PmgdYGuuI7pLarFS ubuntu_train_subtask_1.json\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YXsj5U-P1nj3ID2ni62vvW4m_4AMYh_v\n",
            "To: /content/dstc7/ubuntu_dev_subtask_1.json\n",
            "100% 92.8M/92.8M [00:00<00:00, 118MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1I46tBMSYrDCFb0UoequBSl4Uqo0GOVms\n",
            "To: /content/dstc7/ubuntu_responses_subtask_1.tsv\n",
            "100% 101k/101k [00:00<00:00, 113MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tHAe_WGFqQUHmdM28RttlVsYukL69qtg\n",
            "To: /content/dstc7/ubuntu_test_subtask_1.json\n",
            "100% 18.4M/18.4M [00:00<00:00, 102MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1s2Fz0wQD-YL0tR14PmgdYGuuI7pLarFS\n",
            "To: /content/dstc7/ubuntu_train_subtask_1.json\n",
            "100% 1.86G/1.86G [00:09<00:00, 188MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Poly-Encoder/bert_model\n",
        "!bash run.sh\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6xv5gdygBZw",
        "outputId": "e24eadda-d770-4de1-9573-16fe85b9c362"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Poly-Encoder/bert_model\n",
            "Building PyTorch model from configuration: BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Converting TensorFlow checkpoint from /content/Poly-Encoder/bert_model/bert_model.ckpt\n",
            "Loading TF weight bert/embeddings/LayerNorm/beta with shape [512]\n",
            "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight bert/embeddings/position_embeddings with shape [512, 512]\n",
            "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 512]\n",
            "Loading TF weight bert/embeddings/word_embeddings with shape [30522, 512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [2048]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [512, 2048]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [512]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [2048, 512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [2048]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [512, 2048]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [512]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [2048, 512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [2048]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [512, 2048]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [512]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [2048, 512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [2048]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [512, 2048]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [512]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [2048, 512]\n",
            "Loading TF weight bert/pooler/dense/bias with shape [512]\n",
            "Loading TF weight bert/pooler/dense/kernel with shape [512, 512]\n",
            "Loading TF weight cls/predictions/output_bias with shape [30522]\n",
            "Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [512]\n",
            "Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight cls/predictions/transform/dense/bias with shape [512]\n",
            "Loading TF weight cls/predictions/transform/dense/kernel with shape [512, 512]\n",
            "Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
            "Loading TF weight cls/seq_relationship/output_weights with shape [2, 512]\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n",
            "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n",
            "Save PyTorch model to .//pytorch_model.bin\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Poly-Encoder/dstc7\n",
        "!bash parse.sh\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGQaVPejhcsG",
        "outputId": "d5af3f64-a8c2-46d0-9345-5c0cb03ad2a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Poly-Encoder/dstc7\n",
            "tcmalloc: large alloc 1859141632 bytes == 0x2786000 @  0x7f34204271e7 0x4a3940 0x5b438c 0x5ea94f 0x5939cb 0x594cd3 0x5d0ecb 0x5939af 0x594cd3 0x594f8e 0x59526e 0x5bfba0 0x59aeca 0x515655 0x549e0e 0x593fce 0x548ae9 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7f3420024c87 0x5b621a\n",
            "tcmalloc: large alloc 1859141632 bytes == 0x7148a000 @  0x7f34204271e7 0x4a3940 0x52ab72 0x527cf3 0x51d358 0x59358d 0x548c51 0x51566f 0x549576 0x4bcb19 0x59c019 0x59588e 0x595e64 0x4d8924 0x5bfbcb 0x59aeca 0x515655 0x549e0e 0x593fce 0x548ae9 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7f3420024c87 0x5b621a\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Poly-Encoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrQapiRNjwOU",
        "outputId": "3bfe1cf9-d066-4ecb-f45b-f956f434b696"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Poly-Encoder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --bert_model bert_model/ --output_dir output_dstc7/ --train_dir dstc7/ --use_pretrain --architecture bi\n",
        "# !python3 run.py --bert_model bert_model/ --output_dir output_dstc7/ --train_dir dstc7/ --use_pretrain --architecture poly --poly_m 16\n",
        "# !python3 run.py --bert_model bert_model/ --output_dir output_dstc7/ --train_dir dstc7/ --use_pretrain --architecture cross"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch5QnDV6jVZj",
        "outputId": "813a811e-f947-491e-c7a3-f7236b3d0ad7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(adam_epsilon=1e-08, architecture='bi', bert_model='bert_model/', eval=False, eval_batch_size=32, fp16=False, fp16_opt_level='O1', gpu=0, gradient_accumulation_steps=1, learning_rate=5e-05, max_contexts_length=128, max_grad_norm=1.0, max_response_length=32, model_type='bert', num_train_epochs=10.0, output_dir='output_dstc7/', poly_m=0, print_freq=100, seed=12345, train_batch_size=32, train_dir='dstc7/', use_pretrain=True, warmup_steps=100, weight_decay=0.01)\n",
            "================================================================================\n",
            "Train dir: dstc7/\n",
            "Output dir: output_dstc7/\n",
            "================================================================================\n",
            "Loading parameters from bert_model/pytorch_model.bin\n",
            "Some weights of the model checkpoint at bert_model/ were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "Print freq: 100 Eval freq: 1000\n",
            "  3% 100/3125 [00:18<09:13,  5.47it/s]100 13.571730039119721\n",
            "  6% 200/3125 [00:36<08:46,  5.56it/s]200 8.412441533803939\n",
            " 10% 300/3125 [00:53<08:22,  5.62it/s]300 6.545246367454529\n",
            " 13% 400/3125 [01:10<07:58,  5.69it/s]400 5.58800742983818\n",
            " 16% 500/3125 [01:28<07:37,  5.73it/s]500 4.99717125916481\n",
            " 19% 600/3125 [01:47<07:39,  5.50it/s]600 4.60038396179676\n",
            " 22% 700/3125 [02:08<07:43,  5.23it/s]700 4.294534110682351\n",
            " 26% 800/3125 [02:29<07:38,  5.07it/s]800 4.074664087295532\n",
            " 29% 900/3125 [02:50<07:26,  4.98it/s]900 3.8960051169660357\n",
            " 32% 1000/3125 [03:11<07:11,  4.93it/s]1000 3.7517764822244644\n",
            "Global Step 1000 VAL res:\n",
            " {'train_loss': 3.7517764822244644, 'eval_loss': 3.1897413954138756, 'R1': 0.332, 'R2': 0.394, 'R5': 0.515, 'R10': 0.604, 'MRR': 0.4247505026126117, 'epoch': 1, 'global_step': 1000}\n",
            "[Saving at] output_dstc7/bi_0_pytorch_model.bin\n",
            " 35% 1100/3125 [04:00<09:50,  3.43it/s]1100 3.629265968799591\n",
            " 38% 1200/3125 [04:20<08:29,  3.78it/s]1200 3.527230738500754\n",
            " 42% 1300/3125 [04:41<07:29,  4.06it/s]1300 3.4372618936575376\n",
            " 45% 1400/3125 [05:01<06:41,  4.29it/s]1400 3.3562978317056382\n",
            " 48% 1500/3125 [05:21<06:03,  4.47it/s]1500 3.2914089992841085\n",
            " 51% 1600/3125 [05:42<05:31,  4.60it/s]1600 3.2306373140960933\n",
            " 54% 1700/3125 [06:02<05:02,  4.71it/s]1700 3.1751634369176975\n",
            " 58% 1800/3125 [06:22<04:36,  4.79it/s]1800 3.124394140574667\n",
            " 61% 1900/3125 [06:42<04:12,  4.85it/s]1900 3.0794818097039274\n",
            " 64% 2000/3125 [07:02<03:49,  4.89it/s]2000 3.0365415125489235\n",
            "Global Step 2000 VAL res:\n",
            " {'train_loss': 3.0365415125489235, 'eval_loss': 3.0139527767896652, 'R1': 0.352, 'R2': 0.439, 'R5': 0.551, 'R10': 0.648, 'MRR': 0.45296734095592733, 'epoch': 1, 'global_step': 2000}\n",
            "[Saving at] output_dstc7/bi_0_pytorch_model.bin\n",
            " 67% 2100/3125 [07:52<05:02,  3.39it/s]2100 2.993749990633556\n",
            " 70% 2200/3125 [08:12<04:05,  3.77it/s]2200 2.9576360243017024\n",
            " 74% 2300/3125 [08:32<03:22,  4.08it/s]2300 2.9255412557850713\n",
            " 77% 2400/3125 [08:51<02:46,  4.34it/s]2400 2.8954501922925315\n",
            " 80% 2500/3125 [09:11<02:17,  4.54it/s]2500 2.866195161151886\n",
            " 83% 2600/3125 [09:31<01:51,  4.69it/s]2600 2.8381727487307327\n",
            " 86% 2700/3125 [09:50<01:28,  4.80it/s]2700 2.8134382242626614\n",
            " 90% 2800/3125 [10:10<01:06,  4.89it/s]2800 2.789573505776269\n",
            " 93% 2900/3125 [10:30<00:45,  4.95it/s]2900 2.7643370888150973\n",
            " 96% 3000/3125 [10:49<00:25,  5.00it/s]3000 2.740389097968737\n",
            "Global Step 3000 VAL res:\n",
            " {'train_loss': 2.740389097968737, 'eval_loss': 2.8183560110628605, 'R1': 0.383, 'R2': 0.488, 'R5': 0.582, 'R10': 0.665, 'MRR': 0.48478330254548846, 'epoch': 1, 'global_step': 3000}\n",
            "[Saving at] output_dstc7/bi_0_pytorch_model.bin\n",
            " 99% 3100/3125 [11:37<00:07,  3.52it/s]3100 2.719608185483563\n",
            " 99% 3100/3125 [11:41<00:05,  4.42it/s]\n",
            "Epoch 1, Global Step 3125 VAL res:\n",
            " {'train_loss': 2.7144490142822266, 'eval_loss': 2.839960180222988, 'R1': 0.376, 'R2': 0.476, 'R5': 0.586, 'R10': 0.68, 'MRR': 0.4803161471672755, 'epoch': 1, 'global_step': 3125}\n",
            "3125 2.7144490142822266\n",
            "  3% 100/3125 [00:17<08:42,  5.79it/s]3225 1.8288913512229918\n",
            "  6% 200/3125 [00:34<08:24,  5.80it/s]3325 1.8470110565423965\n",
            " 10% 300/3125 [00:53<08:33,  5.50it/s]3425 1.8322181963920594\n",
            " 13% 400/3125 [01:13<08:27,  5.37it/s]3525 1.8363841924071311\n",
            " 16% 500/3125 [01:32<08:15,  5.30it/s]3625 1.8391437394618988\n",
            " 19% 600/3125 [01:52<08:03,  5.23it/s]3725 1.8414369342724481\n",
            " 22% 700/3125 [02:11<07:44,  5.22it/s]3825 1.8409987345763614\n",
            " 26% 800/3125 [02:30<07:25,  5.22it/s]3925 1.8403005777299404\n",
            "Global Step 4000 VAL res:\n",
            " {'train_loss': 1.8406075060708182, 'eval_loss': 2.7597117573022842, 'R1': 0.384, 'R2': 0.5, 'R5': 0.615, 'R10': 0.713, 'MRR': 0.49751720944578287, 'epoch': 2, 'global_step': 4000}\n",
            "[Saving at] output_dstc7/bi_0_pytorch_model.bin\n",
            " 29% 900/3125 [03:18<10:25,  3.56it/s]4025 1.837944534884559\n",
            " 32% 1000/3125 [03:37<08:57,  3.95it/s]4125 1.8380642393827438\n",
            " 35% 1100/3125 [03:56<07:53,  4.28it/s]4225 1.8371518615159121\n",
            " 38% 1200/3125 [04:15<07:03,  4.54it/s]4325 1.8377049753069878\n",
            " 42% 1300/3125 [04:34<06:24,  4.75it/s]4425 1.8381061975772564\n",
            " 45% 1400/3125 [04:52<05:50,  4.92it/s]4525 1.8383635889632362\n",
            " 48% 1500/3125 [05:11<05:21,  5.05it/s]4625 1.8341617086728415\n",
            " 51% 1600/3125 [05:30<04:57,  5.13it/s]4725 1.8326683957874774\n",
            " 54% 1700/3125 [05:48<04:34,  5.20it/s]4825 1.8333100030001472\n",
            " 58% 1800/3125 [06:07<04:12,  5.25it/s]4925 1.8329932808213765\n",
            "Global Step 5000 VAL res:\n",
            " {'train_loss': 1.8339720300038655, 'eval_loss': 2.7046801932156086, 'R1': 0.403, 'R2': 0.494, 'R5': 0.603, 'R10': 0.703, 'MRR': 0.502804696996922, 'epoch': 2, 'global_step': 5000}\n",
            "[Saving at] output_dstc7/bi_0_pytorch_model.bin\n",
            " 61% 1900/3125 [06:55<05:40,  3.59it/s]5025 1.8331628084182738\n",
            " 64% 2000/3125 [07:13<04:38,  4.04it/s]5125 1.831067921578884\n",
            " 67% 2100/3125 [07:30<03:51,  4.43it/s]5225 1.8302301018578666\n",
            " 70% 2200/3125 [07:47<03:14,  4.76it/s]5325 1.8292580275643955\n",
            " 74% 2300/3125 [08:05<02:44,  5.02it/s]5425 1.8288097034329953\n",
            " 77% 2400/3125 [08:22<02:19,  5.20it/s]5525 1.8246303508182367\n",
            " 80% 2500/3125 [08:40<01:56,  5.35it/s]5625 1.8236798286914826\n",
            " 83% 2600/3125 [08:57<01:36,  5.47it/s]5725 1.8215772797969672\n",
            " 86% 2700/3125 [09:15<01:17,  5.49it/s]5825 1.8214186748751888\n",
            " 90% 2800/3125 [09:34<00:59,  5.47it/s]5925 1.8182556068045752\n",
            "Global Step 6000 VAL res:\n",
            " {'train_loss': 1.8162921385972397, 'eval_loss': 2.5889941677451134, 'R1': 0.428, 'R2': 0.516, 'R5': 0.642, 'R10': 0.727, 'MRR': 0.5273776126510793, 'epoch': 2, 'global_step': 6000}\n",
            "[Saving at] output_dstc7/bi_0_pytorch_model.bin\n",
            " 93% 2900/3125 [10:21<01:00,  3.70it/s]6025 1.8156042013086122\n",
            " 96% 3000/3125 [10:40<00:30,  4.09it/s]6125 1.8131815887093543\n",
            " 99% 3100/3125 [10:59<00:05,  4.39it/s]6225 1.8114218735118066\n",
            " 99% 3100/3125 [11:03<00:05,  4.67it/s]\n",
            "Epoch 2, Global Step 6250 VAL res:\n",
            " {'train_loss': 1.8110347936058044, 'eval_loss': 2.629047852009535, 'R1': 0.418, 'R2': 0.509, 'R5': 0.635, 'R10': 0.725, 'MRR': 0.5206939752640921, 'epoch': 2, 'global_step': 6250}\n",
            "6250 1.8110347936058044\n",
            "  3% 100/3125 [00:17<08:41,  5.80it/s]6350 1.452624039053917\n",
            "  6% 200/3125 [00:34<08:24,  5.79it/s]6450 1.4785789528489113\n",
            " 10% 300/3125 [00:51<08:08,  5.78it/s]6550 1.4893456371625264\n",
            " 13% 400/3125 [01:09<07:50,  5.79it/s]6650 1.4886369083821773\n",
            " 16% 500/3125 [01:26<07:31,  5.81it/s]6750 1.4915720502138137\n",
            " 19% 600/3125 [01:43<07:14,  5.82it/s]6850 1.494795747101307\n",
            " 22% 700/3125 [02:00<06:57,  5.81it/s]6950 1.4931151184865405\n",
            "Global Step 7000 VAL res:\n",
            " {'train_loss': 1.4984088761806489, 'eval_loss': 2.644649613648653, 'R1': 0.427, 'R2': 0.522, 'R5': 0.645, 'R10': 0.734, 'MRR': 0.5303969419216235, 'epoch': 3, 'global_step': 7000}\n",
            " 26% 800/3125 [02:46<10:11,  3.80it/s]7050 1.495532714277506\n",
            " 29% 900/3125 [03:04<08:46,  4.22it/s]7150 1.5019147819942897\n",
            " 32% 1000/3125 [03:22<07:46,  4.56it/s]7250 1.4965590144395828\n",
            " 35% 1100/3125 [03:40<07:00,  4.82it/s]7350 1.4949256213686684\n",
            " 38% 1200/3125 [03:58<06:22,  5.03it/s]7450 1.4961259627342225\n",
            " 42% 1300/3125 [04:16<05:52,  5.18it/s]7550 1.4982410242924324\n",
            " 45% 1400/3125 [04:34<05:25,  5.29it/s]7650 1.5014920765161515\n",
            " 48% 1500/3125 [04:52<05:02,  5.37it/s]7750 1.502930943330129\n",
            " 51% 1600/3125 [05:10<04:40,  5.43it/s]7850 1.5035321112722158\n",
            " 54% 1700/3125 [05:27<04:19,  5.48it/s]7950 1.5024736006119672\n",
            "Global Step 8000 VAL res:\n",
            " {'train_loss': 1.5016257216589792, 'eval_loss': 2.5875454246997833, 'R1': 0.432, 'R2': 0.526, 'R5': 0.633, 'R10': 0.741, 'MRR': 0.5330863544237353, 'epoch': 3, 'global_step': 8000}\n",
            "[Saving at] output_dstc7/bi_0_pytorch_model.bin\n",
            " 58% 1800/3125 [06:15<05:57,  3.71it/s]8050 1.5011884033679963\n",
            " 61% 1900/3125 [06:33<04:56,  4.13it/s]8150 1.5008487876151737\n",
            " 64% 2000/3125 [06:50<04:10,  4.48it/s]8250 1.5009860382676126\n",
            " 67% 2100/3125 [07:08<03:34,  4.77it/s]8350 1.503112175975527\n",
            " 70% 2200/3125 [07:26<03:05,  4.99it/s]8450 1.5056458005580036\n",
            " 74% 2300/3125 [07:44<02:39,  5.16it/s]8550 1.505378701194473\n",
            " 77% 2400/3125 [08:02<02:17,  5.28it/s]8650 1.5055848495165507\n",
            " 80% 2500/3125 [08:20<01:56,  5.37it/s]8750 1.5058830609083176\n",
            " 83% 2600/3125 [08:38<01:36,  5.43it/s]8850 1.506193943803127\n",
            " 86% 2700/3125 [08:56<01:17,  5.48it/s]8950 1.5059917983081605\n",
            "Global Step 9000 VAL res:\n",
            " {'train_loss': 1.5061790179122578, 'eval_loss': 2.5311031453311443, 'R1': 0.44, 'R2': 0.536, 'R5': 0.656, 'R10': 0.749, 'MRR': 0.5425868814082601, 'epoch': 3, 'global_step': 9000}\n",
            "[Saving at] output_dstc7/bi_0_pytorch_model.bin\n",
            " 90% 2800/3125 [09:58<01:42,  3.17it/s]9050 1.5053215889206955\n",
            " 93% 2900/3125 [10:16<01:01,  3.66it/s]9150 1.5047079460785306\n",
            " 96% 3000/3125 [10:33<00:30,  4.09it/s]9250 1.5045845820705095\n",
            " 99% 3100/3125 [10:51<00:05,  4.45it/s]9350 1.5047386050416578\n",
            " 99% 3100/3125 [10:56<00:05,  4.73it/s]\n",
            "Epoch 3, Global Step 9375 VAL res:\n",
            " {'train_loss': 1.5057830925559998, 'eval_loss': 2.5012777112424374, 'R1': 0.446, 'R2': 0.531, 'R5': 0.659, 'R10': 0.747, 'MRR': 0.5449461371387436, 'epoch': 3, 'global_step': 9375}\n",
            "[Saving at] output_dstc7/bi_0_pytorch_model.bin\n",
            "9375 1.5057830925559998\n",
            "  3% 100/3125 [00:17<08:44,  5.77it/s]9475 1.2384413594007493\n",
            "  6% 200/3125 [00:34<08:25,  5.79it/s]9575 1.256750482916832\n",
            " 10% 300/3125 [00:51<08:08,  5.79it/s]9675 1.2577847478787105\n",
            " 13% 400/3125 [01:09<07:50,  5.80it/s]9775 1.2670501278340816\n",
            " 16% 500/3125 [01:26<07:33,  5.79it/s]9875 1.263414671421051\n",
            " 19% 600/3125 [01:43<07:14,  5.82it/s]9975 1.2701090648770332\n",
            "Global Step 10000 VAL res:\n",
            " {'train_loss': 1.2739633338928222, 'eval_loss': 2.5367139652371407, 'R1': 0.45, 'R2': 0.54, 'R5': 0.649, 'R10': 0.737, 'MRR': 0.5475422685001902, 'epoch': 4, 'global_step': 10000}\n",
            " 22% 700/3125 [02:28<10:39,  3.79it/s]10075 1.2689562196390969\n",
            " 26% 800/3125 [02:45<09:05,  4.26it/s]10175 1.2710889033228159\n",
            " 29% 900/3125 [03:03<07:58,  4.65it/s]10275 1.2722867162360085\n",
            " 32% 1000/3125 [03:20<07:08,  4.96it/s]10375 1.2700139663219452\n",
            " 35% 1100/3125 [03:37<06:29,  5.20it/s]10475 1.2730735583738848\n",
            " 38% 1200/3125 [03:54<05:57,  5.38it/s]10575 1.272766955991586\n",
            " 42% 1300/3125 [04:11<05:31,  5.50it/s]10675 1.2744880843162536\n",
            " 45% 1400/3125 [04:29<05:09,  5.58it/s]10775 1.2748419301424707\n",
            " 48% 1500/3125 [04:46<04:48,  5.64it/s]10875 1.2751033019224802\n",
            " 51% 1600/3125 [05:03<04:28,  5.69it/s]10975 1.2729867029562592\n",
            "Global Step 11000 VAL res:\n",
            " {'train_loss': 1.2716744679671068, 'eval_loss': 2.5677478201687336, 'R1': 0.439, 'R2': 0.529, 'R5': 0.65, 'R10': 0.751, 'MRR': 0.5414607552385721, 'epoch': 4, 'global_step': 11000}\n",
            " 54% 1700/3125 [05:49<06:12,  3.82it/s]11075 1.2731183807639515\n",
            " 58% 1800/3125 [06:06<05:10,  4.26it/s]11175 1.2739216905832291\n",
            " 61% 1900/3125 [06:24<04:24,  4.63it/s]11275 1.2757430652568216\n",
            " 64% 2000/3125 [06:41<03:47,  4.94it/s]11375 1.2757830908298493\n",
            " 67% 2100/3125 [06:58<03:18,  5.17it/s]11475 1.2769624840929394\n",
            " 70% 2200/3125 [07:15<02:52,  5.36it/s]11575 1.276536101265387\n",
            " 74% 2300/3125 [07:32<02:29,  5.50it/s]11675 1.2781735611998517\n",
            " 77% 2400/3125 [07:49<02:09,  5.60it/s]11775 1.278281829232971\n",
            " 80% 2500/3125 [08:06<01:50,  5.66it/s]11875 1.2782084985256195\n",
            " 83% 2600/3125 [08:24<01:31,  5.71it/s]11975 1.2794339796671501\n",
            "Global Step 12000 VAL res:\n",
            " {'train_loss': 1.2782945064363025, 'eval_loss': 2.512604508548975, 'R1': 0.462, 'R2': 0.55, 'R5': 0.657, 'R10': 0.755, 'MRR': 0.5589844256622255, 'epoch': 4, 'global_step': 12000}\n",
            " 86% 2700/3125 [09:09<01:49,  3.88it/s]12075 1.2795204982934174\n",
            " 90% 2800/3125 [09:26<01:15,  4.31it/s]12175 1.2804205776325295\n",
            " 93% 2900/3125 [09:43<00:48,  4.68it/s]12275 1.279259900985093\n",
            " 96% 3000/3125 [10:00<00:25,  4.98it/s]12375 1.2796054047544798\n",
            " 99% 3100/3125 [10:17<00:04,  5.21it/s]12475 1.2802593556142623\n",
            " 99% 3100/3125 [10:22<00:05,  4.98it/s]\n",
            "Epoch 4, Global Step 12500 VAL res:\n",
            " {'train_loss': 1.2799328974342346, 'eval_loss': 2.5640233494341373, 'R1': 0.449, 'R2': 0.553, 'R5': 0.668, 'R10': 0.769, 'MRR': 0.5540865835214488, 'epoch': 4, 'global_step': 12500}\n",
            "12500 1.2799328974342346\n",
            "  3% 100/3125 [00:17<08:44,  5.76it/s]12600 1.0426703932881356\n",
            "  6% 200/3125 [00:34<08:24,  5.79it/s]12700 1.065064624994993\n",
            " 10% 300/3125 [00:51<08:06,  5.81it/s]12800 1.0767946476737658\n",
            " 13% 400/3125 [01:08<07:47,  5.83it/s]12900 1.0817058730870486\n",
            " 16% 500/3125 [01:25<07:30,  5.83it/s]13000 1.093292308986187\n",
            "Global Step 13000 VAL res:\n",
            " {'train_loss': 1.093292308986187, 'eval_loss': 2.6533002257347107, 'R1': 0.437, 'R2': 0.532, 'R5': 0.642, 'R10': 0.752, 'MRR': 0.5402743119138136, 'epoch': 5, 'global_step': 13000}\n",
            " 19% 600/3125 [02:10<11:12,  3.76it/s]13100 1.0885800962150096\n",
            " 22% 700/3125 [02:28<09:32,  4.24it/s]13200 1.0854058103050503\n",
            " 26% 800/3125 [02:45<08:20,  4.64it/s]13300 1.0873417284339666\n",
            " 29% 900/3125 [03:02<07:28,  4.97it/s]13400 1.0827624198463228\n",
            " 32% 1000/3125 [03:19<06:46,  5.22it/s]13500 1.08257623898983\n",
            " 35% 1100/3125 [03:36<06:14,  5.41it/s]13600 1.0870216863805597\n",
            " 38% 1200/3125 [03:56<06:06,  5.26it/s]13700 1.0873521996289492\n",
            " 42% 1300/3125 [04:32<07:21,  4.14it/s]13800 1.0875795812560962\n",
            " 45% 1400/3125 [05:08<07:58,  3.61it/s]13900 1.0880316043751581\n",
            " 48% 1500/3125 [05:44<08:11,  3.31it/s]14000 1.0857946106592815\n",
            "Global Step 14000 VAL res:\n",
            " {'train_loss': 1.0857946106592815, 'eval_loss': 2.618437595665455, 'R1': 0.453, 'R2': 0.553, 'R5': 0.66, 'R10': 0.763, 'MRR': 0.5560851618062607, 'epoch': 5, 'global_step': 14000}\n",
            " 51% 1600/3125 [07:06<11:39,  2.18it/s]14100 1.086156267747283\n",
            " 54% 1700/3125 [07:42<10:11,  2.33it/s]14200 1.0885298241236632\n",
            " 58% 1800/3125 [08:18<09:00,  2.45it/s]14300 1.087940952612294\n",
            " 61% 1900/3125 [08:54<08:01,  2.54it/s]14400 1.0862214068362588\n",
            " 64% 2000/3125 [09:30<07:10,  2.61it/s]14500 1.08853822812438\n",
            " 67% 2100/3125 [10:06<06:25,  2.66it/s]14600 1.0901793998196012\n",
            " 70% 2200/3125 [10:42<05:42,  2.70it/s]14700 1.0931038191372697\n",
            " 74% 2300/3125 [11:18<05:03,  2.72it/s]14800 1.092560143561467\n",
            " 77% 2400/3125 [11:54<04:24,  2.74it/s]14900 1.0935731287673116\n",
            " 80% 2500/3125 [12:30<03:47,  2.74it/s]15000 1.0934097954154014\n",
            "Global Step 15000 VAL res:\n",
            " {'train_loss': 1.0934097954154014, 'eval_loss': 2.5642396360635757, 'R1': 0.458, 'R2': 0.556, 'R5': 0.675, 'R10': 0.769, 'MRR': 0.5606416990264372, 'epoch': 5, 'global_step': 15000}\n",
            " 83% 2600/3125 [13:53<04:23,  1.99it/s]15100 1.094038149588383\n",
            " 86% 2700/3125 [14:28<03:15,  2.18it/s]15200 1.0943820561854927\n",
            " 90% 2800/3125 [15:04<02:19,  2.33it/s]15300 1.096044352427125\n",
            " 93% 2900/3125 [15:40<01:31,  2.45it/s]15400 1.096905066185984\n",
            " 96% 3000/3125 [16:16<00:49,  2.54it/s]15500 1.0987328791419666\n",
            " 99% 3100/3125 [16:52<00:09,  2.61it/s]15600 1.0977756217025942\n",
            " 99% 3100/3125 [17:01<00:08,  3.03it/s]\n",
            "Epoch 5, Global Step 15625 VAL res:\n",
            " {'train_loss': 1.0970307064819336, 'eval_loss': 2.6434300541877747, 'R1': 0.442, 'R2': 0.545, 'R5': 0.67, 'R10': 0.763, 'MRR': 0.549863578880342, 'epoch': 5, 'global_step': 15625}\n",
            "15625 1.0970307064819336\n",
            "  3% 100/3125 [00:17<08:41,  5.80it/s]15725 0.9213831168413162\n",
            "  6% 200/3125 [00:34<08:21,  5.83it/s]15825 0.9090042369067669\n",
            " 10% 300/3125 [00:51<08:03,  5.84it/s]15925 0.9047710396846136\n",
            "Global Step 16000 VAL res:\n",
            " {'train_loss': 0.9234492330551147, 'eval_loss': 2.7245793975889683, 'R1': 0.444, 'R2': 0.553, 'R5': 0.667, 'R10': 0.762, 'MRR': 0.5517525587451048, 'epoch': 6, 'global_step': 16000}\n",
            " 13% 400/3125 [01:36<12:48,  3.54it/s]16025 0.9219388435781002\n",
            " 16% 500/3125 [01:53<10:34,  4.13it/s]16125 0.9279756587743759\n",
            " 19% 600/3125 [02:18<10:18,  4.08it/s]16225 0.9223166214923064\n",
            " 22% 700/3125 [02:54<11:23,  3.55it/s]16325 0.9226629287430218\n",
            " 26% 800/3125 [03:30<11:50,  3.27it/s]16425 0.9253624822944403\n",
            " 29% 900/3125 [03:53<10:29,  3.54it/s]16525 0.9301705513066716\n",
            " 32% 1000/3125 [04:29<10:49,  3.27it/s]16625 0.9318916165530682\n",
            " 35% 1100/3125 [05:04<10:49,  3.12it/s]16725 0.937273508229039\n",
            " 38% 1200/3125 [05:40<10:38,  3.01it/s]16825 0.9406124855329593\n",
            " 42% 1300/3125 [06:15<10:18,  2.95it/s]16925 0.9384588572382927\n",
            "Global Step 17000 VAL res:\n",
            " {'train_loss': 0.9411837715452368, 'eval_loss': 2.6539364233613014, 'R1': 0.45, 'R2': 0.542, 'R5': 0.67, 'R10': 0.756, 'MRR': 0.5522921705325025, 'epoch': 6, 'global_step': 17000}\n",
            " 45% 1400/3125 [07:33<13:30,  2.13it/s]17025 0.9404975658016546\n",
            " 48% 1500/3125 [07:50<10:17,  2.63it/s]17125 0.9427474803725878\n",
            " 51% 1600/3125 [08:07<08:04,  3.15it/s]17225 0.9439093902520835\n",
            " 54% 1700/3125 [08:25<06:30,  3.64it/s]17325 0.9447338905404595\n",
            " 58% 1800/3125 [08:42<05:23,  4.09it/s]17425 0.9443266803357336\n",
            " 61% 1900/3125 [08:59<04:33,  4.48it/s]17525 0.9419140647587023\n",
            " 64% 2000/3125 [09:17<03:54,  4.79it/s]17625 0.941900680795312\n",
            " 67% 2100/3125 [09:34<03:23,  5.04it/s]17725 0.942106067722752\n",
            " 70% 2200/3125 [09:52<02:56,  5.23it/s]17825 0.9433412242342125\n",
            " 74% 2300/3125 [10:09<02:33,  5.38it/s]17925 0.9433859643728837\n",
            "Global Step 18000 VAL res:\n",
            " {'train_loss': 0.9424749270363858, 'eval_loss': 2.651536598801613, 'R1': 0.45, 'R2': 0.55, 'R5': 0.663, 'R10': 0.763, 'MRR': 0.5529258510179088, 'epoch': 6, 'global_step': 18000}\n",
            " 77% 2400/3125 [11:12<03:50,  3.15it/s]18025 0.9429778454701105\n",
            " 80% 2500/3125 [11:29<02:51,  3.64it/s]18125 0.9447180144906044\n",
            " 83% 2600/3125 [11:46<02:08,  4.10it/s]18225 0.9460131429708921\n",
            " 86% 2700/3125 [12:04<01:34,  4.49it/s]18325 0.9457773772875467\n",
            " 90% 2800/3125 [12:21<01:07,  4.80it/s]18425 0.9451192709271397\n",
            " 93% 2900/3125 [12:38<00:44,  5.06it/s]18525 0.9461683512350608\n",
            " 96% 3000/3125 [12:56<00:23,  5.25it/s]18625 0.9466289300620556\n",
            " 99% 3100/3125 [13:13<00:04,  5.39it/s]18725 0.9446446378673277\n",
            " 99% 3100/3125 [13:18<00:06,  3.88it/s]\n",
            "Epoch 6, Global Step 18750 VAL res:\n",
            " {'train_loss': 0.9446950907325745, 'eval_loss': 2.6987198777496815, 'R1': 0.444, 'R2': 0.553, 'R5': 0.669, 'R10': 0.761, 'MRR': 0.552264513800579, 'epoch': 6, 'global_step': 18750}\n",
            "18750 0.9446950907325745\n",
            "  3% 100/3125 [00:17<08:38,  5.83it/s]18850 0.8173256531357765\n",
            "  6% 200/3125 [00:34<08:21,  5.84it/s]18950 0.7945162980258464\n",
            "Global Step 19000 VAL res:\n",
            " {'train_loss': 0.7924001857042312, 'eval_loss': 2.83044021576643, 'R1': 0.454, 'R2': 0.559, 'R5': 0.677, 'R10': 0.758, 'MRR': 0.5589140718448545, 'epoch': 7, 'global_step': 19000}\n",
            " 10% 300/3125 [01:19<14:08,  3.33it/s]19050 0.8142290976643562\n",
            " 13% 400/3125 [01:36<11:18,  4.02it/s]19150 0.8142582651227712\n",
            " 16% 500/3125 [01:53<09:38,  4.53it/s]19250 0.8164506836533546\n",
            " 19% 600/3125 [02:10<08:34,  4.91it/s]19350 0.811861567646265\n",
            " 22% 700/3125 [02:28<07:48,  5.17it/s]19450 0.8114086025527545\n",
            " 26% 800/3125 [02:45<07:13,  5.37it/s]19550 0.8148196480795741\n",
            " 29% 900/3125 [03:02<06:43,  5.51it/s]19650 0.8140432113409042\n",
            " 32% 1000/3125 [03:19<06:18,  5.62it/s]19750 0.8183219436705113\n",
            " 35% 1100/3125 [03:36<05:55,  5.69it/s]19850 0.8216712676665999\n",
            " 38% 1200/3125 [03:53<05:35,  5.74it/s]19950 0.8189192420989275\n",
            "Global Step 20000 VAL res:\n",
            " {'train_loss': 0.8178735256671905, 'eval_loss': 2.828763298690319, 'R1': 0.436, 'R2': 0.531, 'R5': 0.659, 'R10': 0.755, 'MRR': 0.5428527916199362, 'epoch': 7, 'global_step': 20000}\n",
            " 42% 1300/3125 [04:38<07:51,  3.87it/s]20050 0.8185934304503294\n",
            " 45% 1400/3125 [05:04<07:26,  3.86it/s]20150 0.8186465063478265\n",
            " 48% 1500/3125 [05:39<07:47,  3.48it/s]20250 0.8187982922991117\n",
            " 51% 1600/3125 [06:15<07:49,  3.25it/s]20350 0.8215401950851082\n",
            " 54% 1700/3125 [06:50<07:38,  3.11it/s]20450 0.822302146887078\n",
            " 58% 1800/3125 [07:26<07:19,  3.02it/s]20550 0.823263415131304\n",
            " 61% 1900/3125 [08:01<06:54,  2.95it/s]20650 0.8230842918941849\n",
            " 64% 2000/3125 [08:37<06:25,  2.91it/s]20750 0.8246645529717207\n",
            " 67% 2100/3125 [09:12<05:55,  2.89it/s]20850 0.8247005693969273\n",
            " 70% 2200/3125 [09:48<05:22,  2.86it/s]20950 0.823304771469398\n",
            "Global Step 21000 VAL res:\n",
            " {'train_loss': 0.8236782823271221, 'eval_loss': 2.8200533129274845, 'R1': 0.449, 'R2': 0.546, 'R5': 0.666, 'R10': 0.761, 'MRR': 0.5527509744534357, 'epoch': 7, 'global_step': 21000}\n",
            " 74% 2300/3125 [10:53<06:02,  2.27it/s]21050 0.8223252689708834\n",
            " 77% 2400/3125 [11:28<05:00,  2.41it/s]21150 0.8235543167777359\n",
            " 80% 2500/3125 [12:04<04:07,  2.52it/s]21250 0.8254783295452595\n",
            " 83% 2600/3125 [12:39<03:21,  2.60it/s]21350 0.825699149410312\n",
            " 86% 2700/3125 [13:15<02:39,  2.66it/s]21450 0.8266570829517311\n",
            " 90% 2800/3125 [13:50<01:59,  2.71it/s]21550 0.8259982562437653\n",
            " 93% 2900/3125 [14:26<01:22,  2.74it/s]21650 0.8264562404618181\n",
            " 96% 3000/3125 [15:01<00:45,  2.77it/s]21750 0.8248420793960491\n",
            " 99% 3100/3125 [15:36<00:08,  2.79it/s]21850 0.824395208930777\n",
            " 99% 3100/3125 [15:45<00:07,  3.28it/s]\n",
            "Epoch 7, Global Step 21875 VAL res:\n",
            " {'train_loss': 0.8241346987867355, 'eval_loss': 2.788484498858452, 'R1': 0.45, 'R2': 0.545, 'R5': 0.674, 'R10': 0.761, 'MRR': 0.5531627287336376, 'epoch': 7, 'global_step': 21875}\n",
            "21875 0.8241346987867355\n",
            "  3% 100/3125 [00:17<08:36,  5.85it/s]21975 0.7110405802726746\n",
            "Global Step 22000 VAL res:\n",
            " {'train_loss': 0.7147826173305512, 'eval_loss': 2.89245617762208, 'R1': 0.45, 'R2': 0.542, 'R5': 0.675, 'R10': 0.759, 'MRR': 0.5527249574439208, 'epoch': 8, 'global_step': 22000}\n",
            "  6% 200/3125 [01:02<16:27,  2.96it/s]22075 0.7147904146462679\n",
            " 10% 300/3125 [01:19<12:20,  3.82it/s]22175 0.7147325709958872\n",
            " 13% 400/3125 [01:36<10:15,  4.43it/s]22275 0.7119607996568084\n",
            " 16% 500/3125 [01:53<09:01,  4.85it/s]22375 0.7136193951070309\n",
            " 19% 600/3125 [02:11<08:10,  5.15it/s]22475 0.7080739147712787\n",
            " 22% 700/3125 [02:28<07:32,  5.36it/s]22575 0.7075166644155979\n",
            " 26% 800/3125 [02:45<07:01,  5.52it/s]22675 0.7146254455856978\n",
            " 29% 900/3125 [03:02<06:35,  5.62it/s]22775 0.7117848717338509\n",
            " 32% 1000/3125 [03:21<06:30,  5.44it/s]22875 0.7102149451822043\n",
            " 35% 1100/3125 [03:57<07:56,  4.25it/s]22975 0.710133453390815\n",
            "Global Step 23000 VAL res:\n",
            " {'train_loss': 0.7100695365269979, 'eval_loss': 2.917872991412878, 'R1': 0.445, 'R2': 0.54, 'R5': 0.662, 'R10': 0.759, 'MRR': 0.54783969471416, 'epoch': 8, 'global_step': 23000}\n",
            " 38% 1200/3125 [05:17<13:08,  2.44it/s]23075 0.7090114247053861\n",
            " 42% 1300/3125 [05:53<11:55,  2.55it/s]23175 0.7129026826069905\n",
            " 45% 1400/3125 [06:28<10:56,  2.63it/s]23275 0.7150563893999372\n",
            " 48% 1500/3125 [07:03<10:03,  2.69it/s]23375 0.7163335292140642\n",
            " 51% 1600/3125 [07:38<09:17,  2.74it/s]23475 0.7185324856825173\n",
            " 54% 1700/3125 [08:13<08:34,  2.77it/s]23575 0.7198074789082303\n",
            " 58% 1800/3125 [08:48<07:54,  2.79it/s]23675 0.7204153930644194\n",
            " 61% 1900/3125 [09:23<07:15,  2.81it/s]23775 0.7195149657757659\n",
            " 64% 2000/3125 [09:58<06:37,  2.83it/s]23875 0.7211303498446942\n",
            " 67% 2100/3125 [10:33<06:01,  2.84it/s]23975 0.7200163867218153\n",
            "Global Step 24000 VAL res:\n",
            " {'train_loss': 0.7209596454956952, 'eval_loss': 2.895393617451191, 'R1': 0.448, 'R2': 0.539, 'R5': 0.664, 'R10': 0.759, 'MRR': 0.5500127447199256, 'epoch': 8, 'global_step': 24000}\n",
            " 70% 2200/3125 [11:53<07:30,  2.06it/s]24075 0.7203642749650911\n",
            " 74% 2300/3125 [12:28<06:07,  2.25it/s]24175 0.7190109234141266\n",
            " 77% 2400/3125 [13:03<05:01,  2.40it/s]24275 0.719579395763576\n",
            " 80% 2500/3125 [13:38<04:07,  2.52it/s]24375 0.720150669157505\n",
            " 83% 2600/3125 [14:13<03:20,  2.62it/s]24475 0.7199541236460208\n",
            " 86% 2700/3125 [14:48<02:38,  2.69it/s]24575 0.7218515161562848\n",
            " 90% 2800/3125 [15:22<01:58,  2.74it/s]24675 0.7228211409598589\n",
            " 93% 2900/3125 [15:57<01:20,  2.78it/s]24775 0.7224804665199641\n",
            " 96% 3000/3125 [16:32<00:44,  2.81it/s]24875 0.7223607066969077\n",
            " 99% 3100/3125 [17:07<00:08,  2.83it/s]24975 0.7229621065239753\n",
            "Global Step 25000 VAL res:\n",
            " {'train_loss': 0.7227404034996032, 'eval_loss': 2.8816743940114975, 'R1': 0.446, 'R2': 0.541, 'R5': 0.668, 'R10': 0.76, 'MRR': 0.5497908587824284, 'epoch': 8, 'global_step': 25000}\n",
            " 99% 3100/3125 [18:01<00:08,  2.87it/s]\n",
            "Epoch 8, Global Step 25000 VAL res:\n",
            " {'train_loss': 0.7227404034996032, 'eval_loss': 2.8816743940114975, 'R1': 0.446, 'R2': 0.541, 'R5': 0.668, 'R10': 0.76, 'MRR': 0.5497908587824284, 'epoch': 8, 'global_step': 25000}\n",
            "25000 0.7227404034996032\n",
            "  3% 100/3125 [00:17<08:35,  5.86it/s]25100 0.6503307105600834\n",
            "  6% 200/3125 [00:34<08:18,  5.87it/s]25200 0.6480720023065806\n",
            " 10% 300/3125 [00:51<08:01,  5.87it/s]25300 0.6341565450529257\n",
            " 13% 400/3125 [01:08<07:43,  5.88it/s]25400 0.6408446283265948\n",
            " 16% 500/3125 [01:25<07:26,  5.88it/s]25500 0.6380901338458062\n",
            " 19% 600/3125 [01:41<07:08,  5.89it/s]25600 0.6402920637528101\n",
            " 22% 700/3125 [01:58<06:51,  5.90it/s]25700 0.6365106286747115\n",
            " 26% 800/3125 [02:15<06:33,  5.90it/s]25800 0.6343621853552759\n",
            " 29% 900/3125 [02:32<06:17,  5.89it/s]25900 0.6385246192581123\n",
            " 32% 1000/3125 [02:49<06:00,  5.89it/s]26000 0.6373311235159635\n",
            "Global Step 26000 VAL res:\n",
            " {'train_loss': 0.6373311235159635, 'eval_loss': 3.0064191669225693, 'R1': 0.445, 'R2': 0.536, 'R5': 0.658, 'R10': 0.749, 'MRR': 0.5467153328637989, 'epoch': 9, 'global_step': 26000}\n",
            " 35% 1100/3125 [03:34<08:36,  3.92it/s]26100 0.6373978441140868\n",
            " 38% 1200/3125 [03:51<07:20,  4.37it/s]26200 0.6366416204099854\n",
            " 42% 1300/3125 [04:08<06:25,  4.74it/s]26300 0.6364699234297643\n",
            " 45% 1400/3125 [04:25<05:42,  5.04it/s]26400 0.6370656530559063\n",
            " 48% 1500/3125 [04:42<05:09,  5.25it/s]26500 0.6387195624113083\n",
            " 51% 1600/3125 [04:59<04:40,  5.43it/s]26600 0.6399385653622448\n",
            " 54% 1700/3125 [05:16<04:16,  5.56it/s]26700 0.6418255983556018\n",
            " 58% 1800/3125 [05:33<03:54,  5.65it/s]26800 0.6432476583123207\n",
            " 61% 1900/3125 [05:50<03:34,  5.71it/s]26900 0.6458452135873468\n",
            " 64% 2000/3125 [06:07<03:15,  5.76it/s]27000 0.6458015943616628\n",
            "Global Step 27000 VAL res:\n",
            " {'train_loss': 0.6458015943616628, 'eval_loss': 2.952001206576824, 'R1': 0.442, 'R2': 0.538, 'R5': 0.663, 'R10': 0.754, 'MRR': 0.5470657110470124, 'epoch': 9, 'global_step': 27000}\n",
            " 67% 2100/3125 [06:52<04:23,  3.89it/s]27100 0.6468184405423346\n",
            " 70% 2200/3125 [07:09<03:33,  4.33it/s]27200 0.6458258866654201\n",
            " 74% 2300/3125 [07:26<02:55,  4.70it/s]27300 0.6454125329009864\n",
            " 77% 2400/3125 [07:43<02:25,  5.00it/s]27400 0.6465914012057086\n",
            " 80% 2500/3125 [08:01<01:59,  5.22it/s]27500 0.6474424201905727\n",
            " 83% 2600/3125 [08:18<01:37,  5.39it/s]27600 0.647652550265193\n",
            " 86% 2700/3125 [08:35<01:16,  5.53it/s]27700 0.6477844225532479\n",
            " 90% 2800/3125 [08:52<00:57,  5.63it/s]27800 0.6489547856205277\n",
            " 93% 2900/3125 [09:09<00:39,  5.70it/s]27900 0.6484064368132887\n",
            " 96% 3000/3125 [09:26<00:21,  5.75it/s]28000 0.6484393992722034\n",
            "Global Step 28000 VAL res:\n",
            " {'train_loss': 0.6484393992722034, 'eval_loss': 2.9803047068417072, 'R1': 0.443, 'R2': 0.546, 'R5': 0.662, 'R10': 0.754, 'MRR': 0.5484607127521385, 'epoch': 9, 'global_step': 28000}\n",
            " 99% 3100/3125 [10:11<00:06,  3.88it/s]28100 0.6468094173938997\n",
            " 99% 3100/3125 [10:15<00:04,  5.03it/s]\n",
            "Epoch 9, Global Step 28125 VAL res:\n",
            " {'train_loss': 0.6469432482528686, 'eval_loss': 3.0080268383026123, 'R1': 0.449, 'R2': 0.545, 'R5': 0.664, 'R10': 0.744, 'MRR': 0.5504840697316571, 'epoch': 9, 'global_step': 28125}\n",
            "28125 0.6469432482528686\n",
            "  3% 100/3125 [00:17<08:46,  5.75it/s]28225 0.6014024356007576\n",
            "  6% 200/3125 [00:34<08:24,  5.80it/s]28325 0.6088003887236119\n",
            " 10% 300/3125 [00:51<08:06,  5.81it/s]28425 0.5982950060566267\n",
            " 13% 400/3125 [01:08<07:48,  5.81it/s]28525 0.5908195790275932\n",
            " 16% 500/3125 [01:26<07:31,  5.81it/s]28625 0.5945057748556137\n",
            " 19% 600/3125 [01:43<07:14,  5.82it/s]28725 0.5944916861752669\n",
            " 22% 700/3125 [02:00<06:56,  5.82it/s]28825 0.588150265536138\n",
            " 26% 800/3125 [02:17<06:38,  5.83it/s]28925 0.5909068632125855\n",
            "Global Step 29000 VAL res:\n",
            " {'train_loss': 0.5897392387219837, 'eval_loss': 3.0944519713521004, 'R1': 0.446, 'R2': 0.538, 'R5': 0.664, 'R10': 0.746, 'MRR': 0.5475923470382633, 'epoch': 10, 'global_step': 29000}\n",
            " 29% 900/3125 [03:02<09:37,  3.85it/s]29025 0.5889256768922011\n",
            " 32% 1000/3125 [03:20<08:14,  4.30it/s]29125 0.5914592051878571\n",
            " 35% 1100/3125 [03:37<07:13,  4.67it/s]29225 0.5902265785566785\n",
            " 38% 1200/3125 [03:54<06:28,  4.96it/s]29325 0.5908769987213115\n",
            " 42% 1300/3125 [04:11<05:52,  5.18it/s]29425 0.5920810001687362\n",
            " 45% 1400/3125 [04:29<05:21,  5.36it/s]29525 0.5920074107338276\n",
            " 48% 1500/3125 [04:46<04:56,  5.48it/s]29625 0.5923194824407498\n",
            " 51% 1600/3125 [05:03<04:33,  5.58it/s]29725 0.5917206952860579\n",
            " 54% 1700/3125 [05:21<04:14,  5.61it/s]29825 0.5926334450130953\n",
            " 58% 1800/3125 [05:38<03:54,  5.66it/s]29925 0.5921444493242436\n",
            "Global Step 30000 VAL res:\n",
            " {'train_loss': 0.5924541519125303, 'eval_loss': 3.0796384550631046, 'R1': 0.444, 'R2': 0.544, 'R5': 0.664, 'R10': 0.75, 'MRR': 0.5473791084497236, 'epoch': 10, 'global_step': 30000}\n",
            " 61% 1900/3125 [06:23<05:17,  3.86it/s]30025 0.5919666498781819\n",
            " 64% 2000/3125 [06:40<04:21,  4.30it/s]30125 0.5938658441789448\n",
            " 67% 2100/3125 [06:57<03:39,  4.67it/s]30225 0.5944576029798814\n",
            " 70% 2200/3125 [07:14<03:06,  4.96it/s]30325 0.5938235646181486\n",
            " 74% 2300/3125 [07:31<02:38,  5.20it/s]30425 0.5937996378443812\n",
            " 77% 2400/3125 [07:49<02:14,  5.38it/s]30525 0.5951021213301768\n",
            " 80% 2500/3125 [08:15<02:10,  4.78it/s]30625 0.5949272220820189\n",
            " 83% 2600/3125 [08:50<02:11,  3.99it/s]30725 0.5932840604936848\n",
            " 86% 2700/3125 [09:24<01:58,  3.58it/s]30825 0.592579130511041\n",
            " 90% 2800/3125 [09:59<01:37,  3.33it/s]30925 0.5924382796165134\n",
            "Global Step 31000 VAL res:\n",
            " {'train_loss': 0.5935447934580886, 'eval_loss': 3.073608662933111, 'R1': 0.447, 'R2': 0.544, 'R5': 0.669, 'R10': 0.747, 'MRR': 0.5492928484629115, 'epoch': 10, 'global_step': 31000}\n",
            " 93% 2900/3125 [11:19<01:41,  2.22it/s]31025 0.5938380827754736\n",
            " 96% 3000/3125 [11:54<00:52,  2.38it/s]31125 0.5940995634074012\n",
            " 99% 3100/3125 [12:29<00:09,  2.51it/s]31225 0.5939112534277862\n",
            " 99% 3100/3125 [12:38<00:06,  4.09it/s]\n",
            "Epoch 10, Global Step 31250 VAL res:\n",
            " {'train_loss': 0.5945128940272332, 'eval_loss': 3.070882584899664, 'R1': 0.446, 'R2': 0.544, 'R5': 0.668, 'R10': 0.749, 'MRR': 0.548849017566035, 'epoch': 10, 'global_step': 31250}\n",
            "31250 0.5945128940272332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --bert_model bert_model/ --output_dir output_dstc7/ --train_dir dstc7/ --use_pretrain --architecture bi --eval\n",
        "# !python3 run.py --bert_model bert_model/ --output_dir output_dstc7/ --train_dir dstc7/ --use_pretrain --architecture poly --poly_m 16 --eval\n",
        "# !python3 run.py --bert_model bert_model/ --output_dir output_dstc7/ --train_dir dstc7/ --use_pretrain --architecture cross --eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POq7e5BjjXSr",
        "outputId": "9a69ff75-3599-49b4-f66f-90ea64efee2e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(adam_epsilon=1e-08, architecture='bi', bert_model='bert_model/', eval=True, eval_batch_size=32, fp16=False, fp16_opt_level='O1', gpu=0, gradient_accumulation_steps=1, learning_rate=5e-05, max_contexts_length=128, max_grad_norm=1.0, max_response_length=32, model_type='bert', num_train_epochs=10.0, output_dir='output_dstc7/', poly_m=0, print_freq=100, seed=12345, train_batch_size=32, train_dir='dstc7/', use_pretrain=True, warmup_steps=100, weight_decay=0.01)\n",
            "================================================================================\n",
            "Train dir: dstc7/\n",
            "Output dir: output_dstc7/\n",
            "================================================================================\n",
            "Loading parameters from output_dstc7/bi_0_pytorch_model.bin\n",
            "{'eval_loss': 2.34547645971179, 'R1': 0.481, 'R2': 0.581, 'R5': 0.685, 'R10': 0.77, 'MRR': 0.581299067078589}\n"
          ]
        }
      ]
    }
  ]
}